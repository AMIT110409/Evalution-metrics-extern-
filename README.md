# Evalution-metrics-extern-
Evaluation metrics are used to measure the quality of a machine learning model. They are used to compare different models and to make sure that a model is performing well. There are many different evaluation metrics available, and the best metric to use depends on the specific problem.


Some common evaluation metrics for classification problems include:

Accuracy: This is the percentage of predictions that are correct.
Precision: This is the percentage of positive predictions that are actually positive.
Recall: This is the percentage of actual positives that are predicted as positive.
F1 score: This is a harmonic mean of precision and recall.
Some common evaluation metrics for regression problems include:

Mean squared error (MSE): This is the average squared difference between the predicted values and the actual values.
Mean absolute error (MAE): This is the average absolute difference between the predicted values and the actual values.
Root mean squared error (RMSE): This is the square root of the mean squared error.
It is important to note that no single evaluation metric is perfect. It is often useful to use multiple metrics to get a more complete picture of a model's performance.

Here are some additional details about the evaluation metrics mentioned above:

Accuracy: Accuracy is a simple and intuitive metric, but it can be misleading in some cases. For example, a model that always predicts the majority class will have high accuracy, even if it is not very informative.
Precision: Precision measures how well the model avoids false positives. A high precision model will have few false positives, but it may also have a low recall.
Recall: Recall measures how well the model identifies all of the positive instances. A high recall model will have few false negatives, but it may also have a low precision.
F1 score: The F1 score is a harmonic mean of precision and recall. It is a more balanced metric than either precision or recall, and it is often used as a default metric for evaluating classification models.
Mean squared error (MSE): MSE is a measure of the average squared difference between the predicted values and the actual values. It is a common metric for evaluating regression models.
Mean absolute error (MAE): MAE is a measure of the average absolute difference between the predicted values and the actual values. It is a more robust metric than MSE for datasets with outliers.
Root mean squared error (RMSE): RMSE is the square root of the mean squared error. It is a more intuitive metric than MSE, but it is not as robust to outliers.
When choosing an evaluation metric, it is important to consider the specific problem and the goals of the model. For example, if the goal is to minimize the number of false positives, then precision is an important metric. If the goal is to minimize the number of false negatives, then recall is an important metric. If the goal is to balance precision and recall, then the F1 score is a good metric.

for furthus implement go through the notebook . 
